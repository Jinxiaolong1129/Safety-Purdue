# A0. General
- [2024/08] **[Image-Perfect Imperfections: Safety, Bias, and Authenticity in the Shadow of Text-To-Image Model Evolution](https://arxiv.org/abs/2408.17285)** ![Diffusion](https://img.shields.io/badge/Diffusion-a99cf4) ![CCS'24](https://img.shields.io/badge/CCS'24-f1b800)
- [2024/07] **[Safetywashing: Do AI Safety Benchmarks Actually Measure Safety Progress?](https://arxiv.org/abs/2407.21792v1)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/06] **[Finding Safety Neurons in Large Language Models](https://arxiv.org/abs/2406.14144)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/06] **[SORRY-Bench: Systematically Evaluating Large Language Model Safety Refusal Behaviors](https://arxiv.org/abs/2406.14598)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Benchmark](https://img.shields.io/badge/Benchmark-87b800)
- [2024/06] **[GuardAgent: Safeguard LLM Agents by a Guard Agent via Knowledge-Enabled Reasoning](https://arxiv.org/abs/2406.09187)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Agent](https://img.shields.io/badge/Agent-87b800)
- [2024/06] **[Self and Cross-Model Distillation for LLMs: Effective Methods for Refusal Pattern Alignment](https://arxiv.org/abs/2406.11285)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/06] **[Benchmarking Trustworthiness of Multimodal Large Language Models: A Comprehensive Study](https://arxiv.org/abs/2406.07057)** ![VLM](https://img.shields.io/badge/VLM-c7688b) ![Benchmark](https://img.shields.io/badge/Benchmark-87b800)
- [2024/05] **[AI Risk Management Should Incorporate Both Safety and Security](https://arxiv.org/abs/2405.19524)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/05] **[S-Eval: Automatic and Adaptive Test Generation for Benchmarking Safety Evaluation of Large Language Models](https://arxiv.org/abs/2405.14191)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/04] **[Introducing v0.5 of the AI Safety Benchmark from MLCommons](https://arxiv.org/abs/2404.12241)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/04] **[ALERT: A Comprehensive Benchmark for Assessing Large Language Models' Safety through Red Teaming](https://arxiv.org/abs/2404.08676)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/04] **[Benchmarking Llama2, Mistral, Gemma and GPT for Factuality, Toxicity, Bias and Propensity for Hallucinations](https://arxiv.org/abs/2404.09785)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/04] **[Foundational Challenges in Assuring Alignment and Safety of Large Language Models](https://llm-safety-challenges.github.io/)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Survey](https://img.shields.io/badge/Survey-87b800)
- [2024/04] **[Online Safety Analysis for LLMs: a Benchmark, an Assessment, and a Path Forward](https://arxiv.org/abs/2404.08517)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/04] **[SafetyPrompts: a Systematic Review of Open Datasets for Evaluating and Improving Large Language Model Safety](https://arxiv.org/abs/2404.05399)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
