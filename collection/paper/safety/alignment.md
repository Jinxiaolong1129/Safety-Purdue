# A2. Alignment

## Diffusion
- [2023/09] **[CAS: A Probability-Based Approach for Universal Condition Alignment Score](https://openreview.net/forum?id=E78OaH2s3f)** ![Diffusion](https://img.shields.io/badge/Diffusion-a99cf4) ![ICLR'24_(Spotlight)](https://img.shields.io/badge/ICLR'24_(Spotlight)-f1b800)



## VLM
- [2024/06] **[MOSSBench: Is Your Multimodal Language Model Oversensitive to Safe Queries?](https://arxiv.org/abs/2406.17806)** ![VLM](https://img.shields.io/badge/VLM-c7688b)
- [2024/06] **[Cross-Modality Safety Alignment](https://arxiv.org/abs/2406.15279)** ![VLM](https://img.shields.io/badge/VLM-c7688b)
- [2024/06] **[SPA-VL: A Comprehensive Safety Preference Alignment Dataset for Vision Language Model](https://arxiv.org/abs/2406.12030)** ![VLM](https://img.shields.io/badge/VLM-c7688b) ![Benchmark](https://img.shields.io/badge/Benchmark-87b800)
- [2024/05] **[Safety Alignment for Vision Language Models](https://www.arxiv.org/abs/2405.13581)** ![VLM](https://img.shields.io/badge/VLM-c7688b)
- [2023/09] **[Large Language Models as Automated Aligners for benchmarking Vision-Language Models](https://openreview.net/forum?id=kZEXgtMNNo)** ![VLM](https://img.shields.io/badge/VLM-c7688b) ![ICLR'24](https://img.shields.io/badge/ICLR'24-f1b800)


## LLM
- [2024/10] **[Superficial Safety Alignment Hypothesis](https://arxiv.org/abs/2410.10862)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/10] **[SciSafeEval: A Comprehensive Benchmark for Safety Alignment of Large Language Models in Scientific Tasks](https://arxiv.org/abs/2410.03769)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/09] **[Booster: Tackling Harmful Fine-tuning for Large Language Models via Attenuating Harmful Perturbation](https://arxiv.org/abs/2409.01586)** [<img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" alt="Code" width="15" height="15">](https://github.com/git-disl/Booster) ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/09] **[Harmful Fine-tuning Attacks and Defenses for Large Language Models: A Survey](https://arxiv.org/abs/2409.18169)** [<img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" alt="Code" width="15" height="15">](https://github.com/git-disl/awesome_LLM-harmful-fine-tuning-papers) ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Survey](https://img.shields.io/badge/Survey-87b800)
- [2024/08] **[Safety Layers of Aligned Large Language Models: The Key to LLM Security](https://arxiv.org/abs/2408.17003)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/08] **[Antidote: Post-fine-tuning Safety Alignment for Large Language Models against Harmful Fine-tuning](https://arxiv.org/abs/2408.09600)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/07] **[Can Editing LLMs Inject Harm?](https://arxiv.org/abs/2407.20224)** [<img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" alt="Code" width="15" height="15">](https://llm-editing.github.io/) ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/07] **[The Better Angels of Machine Personality: How Personality Relates to LLM Safety](https://arxiv.org/abs/2407.12344)** ![LLM](https://img.shields.io/badge/LLM-589cf4)

- [2024/06] **[Model Merging and Safety Alignment: One Bad Model Spoils the Bunch](https://arxiv.org/abs/2406.14563)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/06] **[ChatBug: A Common Vulnerability of Aligned LLMs Induced by Chat Templates](https://arxiv.org/abs/2406.12935)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/06] **[Emerging Safety Attack and Defense in Federated Instruction Tuning of Large Language Models](https://arxiv.org/abs/2406.10630)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/06] **[Safety Alignment Should Be Made More Than Just a Few Tokens Deep ](https://arxiv.org/abs/2406.05946)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/06] **[Decoupled Alignment for Robust Plug-and-Play Adaptation](https://arxiv.org/abs/2406.01514)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/05] **[Lazy Safety Alignment for Large Language Models against Harmful Fine-tuning](https://arxiv.org/abs/2405.18641)** [<img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" alt="Code" width="15" height="15">](https://github.com/git-disl/Lisa) ![LLM](https://img.shields.io/badge/LLM-589cf4) ![NeurIPS'24](https://img.shields.io/badge/NeurIPS'24-f1b800)
- [2024/05] **[MoGU: A Framework for Enhancing Safety of Open-Sourced LLMs While Preserving Their Usability](https://arxiv.org/abs/2405.14488)** [<img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" alt="Code" width="15" height="15">](https://github.com/DYR1/MoGU) ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/05] **[Learning diverse attacks on large language models for robust red-teaming and safety tuning](https://arxiv.org/abs/2405.18540)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/05] **[A safety realignment framework via subspace-oriented model fusion for large language models](https://arxiv.org/pdf/2405.09055)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/05] **[A Causal Explainable Guardrails for Large Language Models ](https://arxiv.org/abs/2405.04160)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/04] **[More RLHF, More Trust? On The Impact of Human Preference Alignment On Language Model Trustworthiness](https://arxiv.org/abs/2404.18870)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/03] **[Ensuring Safe and High-Quality Outputs: A Guideline Library Approach for Language Models](https://arxiv.org/abs/2403.11838)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![NAACL'24](https://img.shields.io/badge/NAACL'24-f1b800)
- [2024/03] **[Using Hallucinations to Bypass RLHF Filters](https://arxiv.org/abs/2403.04769)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Attack](https://img.shields.io/badge/Attack-87b800)
- [2024/03] **[Aligners: Decoupling LLMs and Alignment](https://arxiv.org/abs/2403.04224)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![ICLR'24_(Tiny)](https://img.shields.io/badge/ICLR'24_(Tiny)-f1b800)
- [2024/03] **[Negating Negatives: Alignment without Human Positive Samples via Distributional Dispreference Optimization](https://arxiv.org/abs/2403.03419)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/02] **[Vaccine: Perturbation-aware Alignment for Large Language Models against Harmful Fine-tuning](https://arxiv.org/abs/2402.01109)** [<img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" alt="Code" width="15" height="15">](https://github.com/git-disl/Vaccine) ![LLM](https://img.shields.io/badge/LLM-589cf4) ![NeurIPS'24](https://img.shields.io/badge/NeurIPS'24-f1b800)
- [2024/02] **[Keeping LLMs Aligned After Fine-tuning: The Crucial Role of Prompt Templates](https://arxiv.org/abs/2402.18540)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/02] **[Privacy-Preserving Instructions for Aligning Large Language Models](https://arxiv.org/abs/2402.13659)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/02] **[Emulated Disalignment: Safety Alignment for Large Language Models May Backfire!](https://arxiv.org/abs/2402.12343)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/02] **[Language Models are Homer Simpson! Safety Re-Alignment of Fine-tuned Language Models through Task Arithmetic](https://arxiv.org/abs/2402.11746)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/02] **[Learning to Edit: Aligning LLMs with Knowledge Editing](https://arxiv.org/abs/2402.11905)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/02] **[DeAL: Decoding-time Alignment for Large Language Models](https://arxiv.org/abs/2402.06147)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/02] **[Assessing the Brittleness of Safety Alignment via Pruning and Low-Rank Modifications](https://arxiv.org/abs/2402.05162)** [<img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" alt="Code" width="15" height="15">](https://boyiwei.com/alignment-attribution/) ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/01] **[Agent Alignment in Evolving Social Norms](https://arxiv.org/abs/2401.04620)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Agent](https://img.shields.io/badge/Agent-87b800)
- [2023/12] **[Alignment for Honesty](https://arxiv.org/abs/2312.07000)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2023/12] **[Exploiting Novel GPT-4 APIs](https://arxiv.org/abs/2312.14302)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2023/11] **[Removing RLHF Protections in GPT-4 via Fine-Tuning](https://arxiv.org/abs/2311.05553)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2023/10] **[AI Alignment: A Comprehensive Survey](https://arxiv.org/abs/2310.19852)** [<img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" alt="Code" width="15" height="15">](https://github.com/PKU-Alignment/AlignmentSurvey?tab=readme-ov-file) ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Survey](https://img.shields.io/badge/Survey-87b800)
- [2023/10] **[Shadow Alignment: The Ease of Subverting Safely-Aligned Language Models](https://arxiv.org/abs/2310.02949v1)** [<img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" alt="Code" width="15" height="15">](https://github.com/BeyonderXX/ShadowAlignment) ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2023/09] **[Training Socially Aligned Language Models on Simulated Social Interactions](https://openreview.net/forum?id=NddKiWtdUm)** [<img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" alt="Code" width="15" height="15">](https://github.com/agi-templar/Stable-Alignment) ![LLM](https://img.shields.io/badge/LLM-589cf4) ![ICLR'24](https://img.shields.io/badge/ICLR'24-f1b800)
- [2023/09] **[Alignment as Reward-Guided Search](https://openreview.net/forum?id=shgx0eqdw6)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![ICLR'24](https://img.shields.io/badge/ICLR'24-f1b800)
- [2023/09] **[Beyond Imitation: Leveraging Fine-grained Quality Signals for Alignment](https://openreview.net/forum?id=LNLjU5C5dK)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![ICLR'24](https://img.shields.io/badge/ICLR'24-f1b800)
- [2023/09] **[Beyond Reverse KL: Generalizing Direct Preference Optimization with Diverse Divergence Constraints](https://openreview.net/forum?id=2cRzmWXK9N)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![ICLR'24_(Spotlight)](https://img.shields.io/badge/ICLR'24_(Spotlight)-f1b800)
- [2023/09] **[CPPO: Continual Learning for Reinforcement Learning with Human Feedback](https://openreview.net/forum?id=86zAUE80pP)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![ICLR'24](https://img.shields.io/badge/ICLR'24-f1b800)
- [2023/09] **[Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To!](https://openreview.net/forum?id=hTEGyKf0dZ)** [<img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" alt="Code" width="15" height="15">](https://github.com/LLM-Tuning-Safety/LLMs-Finetuning-Safety) ![LLM](https://img.shields.io/badge/LLM-589cf4) ![ICLR'24_(Oral)](https://img.shields.io/badge/ICLR'24_(Oral)-f1b800)
- [2023/09] **[FLASK: Fine-grained Language Model Evaluation based on Alignment Skill Sets](https://openreview.net/forum?id=CYmF38ysDa)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![ICLR'24_(Spotlight)](https://img.shields.io/badge/ICLR'24_(Spotlight)-f1b800)
- [2023/09] **[Gaining Wisdom from Setbacks: Aligning Large Language Models via Mistake Analysis](https://openreview.net/forum?id=aA33A70IO6)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![ICLR'24](https://img.shields.io/badge/ICLR'24-f1b800)
- [2023/09] **[Generative Judge for Evaluating Alignment](https://openreview.net/forum?id=gtkFw6sZGS)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![ICLR'24](https://img.shields.io/badge/ICLR'24-f1b800)
- [2023/09] **[Group Preference Optimization: Few-Shot Alignment of Large Language Models](https://openreview.net/forum?id=DpFeMH4l8Q)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![ICLR'24](https://img.shields.io/badge/ICLR'24-f1b800)
- [2023/09] **[Improving Generalization of Alignment with Human Preferences through Group Invariant Learning](https://openreview.net/forum?id=fwCoLe3TAX)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![ICLR'24_(Spotlight)](https://img.shields.io/badge/ICLR'24_(Spotlight)-f1b800)
- [2023/09] **[Peering Through Preferences: Unraveling Feedback Acquisition for Aligning Large Language Models](https://openreview.net/forum?id=dKl6lMwbCy)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![ICLR'24](https://img.shields.io/badge/ICLR'24-f1b800)
- [2023/09] **[RLCD: Reinforcement Learning from Contrastive Distillation for LM Alignment](https://openreview.net/forum?id=v3XXtxWKi6)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![ICLR'24](https://img.shields.io/badge/ICLR'24-f1b800)
- [2023/09] **[Safe RLHF: Safe Reinforcement Learning from Human Feedback](https://openreview.net/forum?id=TyFrPOKYXw)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![ICLR'24_(Spotlight)](https://img.shields.io/badge/ICLR'24_(Spotlight)-f1b800)
- [2023/09] **[SALMON: Self-Alignment with Principle-Following Reward Models](https://openreview.net/forum?id=xJbsmB8UMx)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![ICLR'24](https://img.shields.io/badge/ICLR'24-f1b800)
- [2023/09] **[Self-Alignment with Instruction Backtranslation](https://openreview.net/forum?id=1oijHJBRsT)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![ICLR'24_(Oral)](https://img.shields.io/badge/ICLR'24_(Oral)-f1b800)
- [2023/09] **[Statistical Rejection Sampling Improves Preference Optimization](https://openreview.net/forum?id=xbjSwwrQOe)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![ICLR'24](https://img.shields.io/badge/ICLR'24-f1b800)
- [2023/09] **[True Knowledge Comes from Practice: Aligning Large Language Models with Embodied Environments via Reinforcement Learning](https://openreview.net/forum?id=hILVmJ4Uvu)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![ICLR'24](https://img.shields.io/badge/ICLR'24-f1b800)
- [2023/09] **[Urial: Aligning Untuned LLMs with Just the 'Write' Amount of In-Context Learning](https://openreview.net/forum?id=wxJ0eXwwda)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![ICLR'24](https://img.shields.io/badge/ICLR'24-f1b800)
- [2023/09] **[What happens when you fine-tuning your model? Mechanistic analysis of procedurally generated tasks.](https://openreview.net/forum?id=A0HKeKl4Nl)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![ICLR'24](https://img.shields.io/badge/ICLR'24-f1b800)
- [2023/09] **[What Makes Good Data for Alignment? A Comprehensive Study of Automatic Data Selection in Instruction Tuning](https://openreview.net/forum?id=BTKAeLqLMw)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![ICLR'24](https://img.shields.io/badge/ICLR'24-f1b800)
- [2023/08] **[Trustworthy LLMs: a Survey and Guideline for Evaluating Large Language Models' Alignment](https://arxiv.org/abs/2308.05374)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Survey](https://img.shields.io/badge/Survey-87b800)
- [2023/07] **[BeaverTails: Towards Improved Safety Alignment of LLM via a Human-Preference Dataset](https://arxiv.org/abs/2307.04657)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2023/07] **[CValues: Measuring the Values of Chinese Large Language Models from Safety to Responsibility](https://arxiv.org/abs/2307.09705)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Chinese](https://img.shields.io/badge/Chinese-87b800)
- [2023/05] **[Principle-Driven Self-Alignment of Language Models from Scratch with Minimal Human Supervision](https://arxiv.org/abs/2305.03047)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2023/04] **[Fundamental Limitations of Alignment in Large Language Models](https://arxiv.org/abs/2304.11082)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2023/04] **[RAFT: Reward rAnked FineTuning for Generative Foundation Model Alignment](https://arxiv.org/abs/2304.06767)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2022/10] **[Enabling Classifiers to Make Judgements Explicitly Aligned with Human Values](https://arxiv.org/abs/2210.07652)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
