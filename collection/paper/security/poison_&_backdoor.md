# B2. Poison & Backdoor

## LLM
- [2024/10] **[Denial-of-Service Poisoning Attacks against Large Language Models](https://arxiv.org/abs/2410.10760)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/10] **[PoisonBench: Assessing Large Language Model Vulnerability to Data Poisoning](https://arxiv.org/abs/2410.08811)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/10] **[ASPIRER: Bypassing System Prompts With Permutation-based Backdoors in LLMs](https://arxiv.org/abs/2410.04009)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/10] **[Controlled Generation of Natural Adversarial Documents for Stealthy Retrieval Poisoning](https://arxiv.org/abs/2410.02163)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![RAG](https://img.shields.io/badge/RAG-87b800)
- [2024/09] **[Mitigating Backdoor Threats to Large Language Models: Advancement and Challenges](https://arxiv.org/abs/2409.19993)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/09] **[CLIBE: Detecting Dynamic Backdoors in Transformer-based NLP Models](https://arxiv.org/abs/2409.01193)** [<img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" alt="Code" width="15" height="15">](https://github.com/Raytsang123/CLIBE) ![LLM](https://img.shields.io/badge/LLM-589cf4) ![NDSS'25](https://img.shields.io/badge/NDSS'25-f1b800)
- [2024/09] **[Weak-To-Strong Backdoor Attacks for LLMs with Contrastive Knowledge Distillation](https://arxiv.org/abs/2409.17946)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/09] **[Obliviate: Neutralizing Task-agnostic Backdoors within the Parameter-efficient Fine-tuning Paradigm](https://arxiv.org/abs/2409.14119)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/09] **[Understanding Implosion in Text-to-Image Generative Models](https://arxiv.org/abs/2409.12314)** ![Diffusion](https://img.shields.io/badge/Diffusion-a99cf4) ![CCS'24](https://img.shields.io/badge/CCS'24-f1b800)
- [2024/09] **[TERD: A Unified Framework for Safeguarding Diffusion Models Against Backdoors ](https://arxiv.org/abs/2409.05294)** ![Diffusion](https://img.shields.io/badge/Diffusion-a99cf4) ![Defense](https://img.shields.io/badge/Defense-87b800) ![ICML'24](https://img.shields.io/badge/ICML'24-f1b800)
- [2024/09] **[The Dark Side of Human Feedback: Poisoning Large Language Models via User Inputs](https://arxiv.org/abs/2409.00787)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/09] **[Exploiting the Vulnerability of Large Language Models via Defense-Aware Architectural Backdoor](https://arxiv.org/abs/2409.01952)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/08] **[Transferring Backdoors between Large Language Models by Knowledge Distillation](https://arxiv.org/abs/2408.09878)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/08] **[Compromising Embodied Agents with Contextual Backdoor Attacks](https://arxiv.org/abs/2408.02882)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Agent](https://img.shields.io/badge/Agent-87b800)
- [2024/08] **[Scaling Laws for Data Poisoning in LLMs](https://arxiv.org/abs/2408.02946)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/07] **[Diff-Cleanse: Identifying and Mitigating Backdoor Attacks in Diffusion Models](https://arxiv.org/abs/2407.21316)** ![Diffusion](https://img.shields.io/badge/Diffusion-a99cf4)
- [2024/07] **[EvilEdit: Backdooring Text-to-Image Diffusion Models in One Second](https://openreview.net/forum?id=ibEaSS6bQn)** [<img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" alt="Code" width="15" height="15">](https://github.com/haowang-cqu/EvilEdit) ![Diffusion](https://img.shields.io/badge/Diffusion-a99cf4) ![MM'24](https://img.shields.io/badge/MM'24-f1b800)
- [2024/07] **[AgentPoison: Red-teaming LLM Agents via Poisoning Memory or Knowledge Bases](https://arxiv.org/abs/2407.12784)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Agent](https://img.shields.io/badge/Agent-87b800)
- [2024/07] **[Future Events as Backdoor Triggers: Investigating Temporal Vulnerabilities in LLMs](https://arxiv.org/abs/2407.04108)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/06] **["Glue pizza and eat rocks" -- Exploiting Vulnerabilities in Retrieval-Augmented Generative Models](https://arxiv.org/pdf/2406.19417)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![RAG](https://img.shields.io/badge/RAG-87b800)
- [2024/06] **[BEEAR: Embedding-based Adversarial Removal of Safety Backdoors in Instruction-tuned Language Models](https://arxiv.org/abs/2406.17092)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/06] **[Injecting Bias in Text-To-Image Models via Composite-Trigger Backdoors](https://arxiv.org/abs/2406.15213)** ![Diffusion](https://img.shields.io/badge/Diffusion-a99cf4)
- [2024/06] **[Is poisoning a real threat to LLM alignment? Maybe more so than you think](https://arxiv.org/abs/2406.12091)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/06] **[CleanGen: Mitigating Backdoor Attacks for Generation Tasks in Large Language Models](https://arxiv.org/abs/2406.12257)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/06] **[Watch the Watcher! Backdoor Attacks on Security-Enhancing Diffusion Models](https://arxiv.org/abs/2406.09669)** ![Diffusion](https://img.shields.io/badge/Diffusion-a99cf4)
- [2024/06] **[An LLM-Assisted Easy-to-Trigger Backdoor Attack on Code Completion Models: Injecting Disguised Vulnerabilities against Strong Detection](https://arxiv.org/abs/2406.06822)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/06] **[A Survey of Backdoor Attacks and Defenses on Large Language Models: Implications for Security Measures](https://arxiv.org/abs/2406.06852)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Survey](https://img.shields.io/badge/Survey-87b800)
- [2024/06] **[Chain-of-Scrutiny: Detecting Backdoor Attacks for Large Language Models](https://arxiv.org/abs/2406.05948)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Defense](https://img.shields.io/badge/Defense-87b800)
- [2024/06] **[BadAgent: Inserting and Activating Backdoor Attacks in LLM Agents](https://arxiv.org/abs/2406.03007)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Agent](https://img.shields.io/badge/Agent-87b800)
- [2024/06] **[Invisible Backdoor Attacks on Diffusion Models](https://arxiv.org/abs/2406.00816)** ![Diffusion](https://img.shields.io/badge/Diffusion-a99cf4)
- [2024/06] **[BadRAG: Identifying Vulnerabilities in Retrieval Augmented Generation of Large Language Models](https://arxiv.org/abs/2406.00083)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![RAG](https://img.shields.io/badge/RAG-87b800)
- [2024/06] **[Are you still on track!? Catching LLM Task Drift with Activations](https://arxiv.org/abs/2406.00799)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Defense](https://img.shields.io/badge/Defense-87b800)
- [2024/05] **[Phantom: General Trigger Attacks on Retrieval Augmented Language Generation](https://arxiv.org/abs/2405.20485)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![RAG](https://img.shields.io/badge/RAG-87b800)
- [2024/05] **[Exploring Backdoor Attacks against Large Language Model-based Decision Making](https://arxiv.org/abs/2405.20774)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![RAG](https://img.shields.io/badge/RAG-87b800)
- [2024/05] **[TrojFM: Resource-efficient Backdoor Attacks against Very Large Foundation Models](https://arxiv.org/pdf/2405.16783)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/05] **[Certifiably Robust RAG against Retrieval Corruption](https://arxiv.org/abs/2405.15556)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![RAG](https://img.shields.io/badge/RAG-87b800)
- [2024/05] **[TrojanRAG: Retrieval-Augmented Generation Can Be Backdoor Driver in Large Language Models](https://arxiv.org/abs/2405.13401)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![RAG](https://img.shields.io/badge/RAG-87b800)
- [2024/05] **[Backdoor Removal for Generative Large Language Models](https://arxiv.org/abs/2405.07667)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/04] **[Transferring Troubles: Cross-Lingual Transferability of Backdoor Attacks in LLMs with Instruction Tunin](https://arxiv.org/abs/2404.19597)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/04] **[Human-Imperceptible Retrieval Poisoning Attacks in LLM-Powered Applications](https://arxiv.org/abs/2404.17196)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![RAG](https://img.shields.io/badge/RAG-87b800)
- [2024/04] **[Talk Too Much: Poisoning Large Language Models under Token Limit](https://arxiv.org/abs/2404.14795)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/04] **[Typos that Broke the RAG's Back: Genetic Attack on RAG Pipeline by Simulating Documents in the Wild via Low-level Perturbations](https://arxiv.org/abs/2404.13948)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![RAG](https://img.shields.io/badge/RAG-87b800)
- [2024/04] **[Physical Backdoor Attack can Jeopardize Driving with Vision-Large-Language Models](https://arxiv.org/abs/2404.12916)** ![VLM](https://img.shields.io/badge/VLM-c7688b)
- [2024/04] **[Best-of-Venom: Attacking RLHF by Injecting Poisoned Preference Data](https://arxiv.org/abs/2404.05530)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/04] **[Shortcuts Arising from Contrast: Effective and Covert Clean-Label Attacks in Prompt-Based Learning](https://arxiv.org/abs/2404.00461)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/04] **[What's in Your "Safe" Data?: Identifying Benign Data that Breaks Safety](https://arxiv.org/abs/2404.01099)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/04] **[UFID: A Unified Framework for Input-level Backdoor Detection on Diffusion Models](https://arxiv.org/abs/2404.01101)** ![Diffusion](https://img.shields.io/badge/Diffusion-a99cf4)
- [2024/03] **[Generating Potent Poisons and Backdoors from Scratch with Guided Diffusion](https://arxiv.org/abs/2403.16365)** [<img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" alt="Code" width="15" height="15">](https://github.com/hsouri/GDP) ![Diffusion](https://img.shields.io/badge/Diffusion-a99cf4)
- [2024/03] **[Diffusion Denoising as a Certified Defense against Clean-label Poisoning ](https://arxiv.org/abs/2403.11981)** ![Diffusion](https://img.shields.io/badge/Diffusion-a99cf4)
- [2024/03] **[LoRA-as-an-Attack! Piercing LLM Safety Under The Share-and-Play Scenario](https://arxiv.org/abs/2403.00108)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/02] **[Syntactic Ghost: An Imperceptible General-purpose Backdoor Attacks on Pre-trained Language Models](https://arxiv.org/abs/2402.18945)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/02] **[On Trojan Signatures in Large Language Models of Code](https://arxiv.org/abs/2402.16896)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/02] **[WIPI: A New Web Threat for LLM-Driven Web Agents](https://arxiv.org/abs/2402.16965)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Agent](https://img.shields.io/badge/Agent-87b800)
- [2024/02] **[VL-Trojan: Multimodal Instruction Backdoor Attacks against Autoregressive Visual Language Models](https://arxiv.org/abs/2402.13851)** ![VLM](https://img.shields.io/badge/VLM-c7688b)
- [2024/02] **[Universal Vulnerabilities in Large Language Models: Backdoor Attacks for In-context Learning](https://arxiv.org/abs/2401.05949)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/02] **[Learning to Poison Large Language Models During Instruction Tuning](https://arxiv.org/abs/2402.13459)** [<img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" alt="Code" width="15" height="15">](https://github.com/RookieZxy/GBTL-attack/tree/main) ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/02] **[Defending Against Weight-Poisoning Backdoor Attacks for Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2402.12168)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Defense](https://img.shields.io/badge/Defense-87b800)
- [2024/02] **[Acquiring Clean Language Models from Backdoor Poisoned Datasets by Downscaling Frequency Space](https://arxiv.org/abs/2402.12026)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/02] **[Rapid Adoption, Hidden Risks: The Dual Impact of Large Language Model Customization](https://arxiv.org/abs/2402.09179)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/02] **[Secret Collusion Among Generative AI Agents](https://arxiv.org/abs/2402.07510v1)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Agent](https://img.shields.io/badge/Agent-87b800)
- [2024/02] **[Test-Time Backdoor Attacks on Multimodal Large Language Models](https://arxiv.org/abs/2402.08577)** [<img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" alt="Code" width="15" height="15">](https://sail-sg.github.io/AnyDoor/) ![VLM](https://img.shields.io/badge/VLM-c7688b)
- [2024/02] **[PoisonedRAG: Knowledge Poisoning Attacks to Retrieval-Augmented Generation of Large Language Models](https://arxiv.org/abs/2402.07867)** [<img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" alt="Code" width="15" height="15">](https://github.com/sleeepeer/PoisonedRAG) ![VLM](https://img.shields.io/badge/VLM-c7688b) ![RAG](https://img.shields.io/badge/RAG-87b800)
- [2024/02] **[Shadowcast: Stealthy Data Poisoning Attacks Against Vision-Language Models](https://vlm-poison.github.io/)** [<img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" alt="Code" width="15" height="15">](https://github.com/umd-huang-lab/VLM-Poisoning) ![VLM](https://img.shields.io/badge/VLM-c7688b)
- [2024/01] **[Universal Vulnerabilities in Large Language Models: In-context Learning Backdoor Attacks](https://arxiv.org/abs/2401.05949)** [<img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" alt="Code" width="15" height="15">](https://github.com/shuaizhao95/ICLAttack) ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/01] **[Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training](https://arxiv.org/abs/2401.05566)** [<img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" alt="Code" width="15" height="15">](https://github.com/anthropics/sleeper-agents-paper) ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2023/12] **[Poisoned ChatGPT Finds Work for Idle Hands: Exploring Developers' Coding Practices with Insecure Suggestions from Poisoned AI Models](https://arxiv.org/abs/2312.06227)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2023/12] **[Stealthy and Persistent Unalignment on Large Language Models via Backdoor Injections](https://arxiv.org/abs/2312.00027)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2023/12] **[Unleashing Cheapfakes through Trojan Plugins of Large Language Models](https://arxiv.org/abs/2312.00374)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2023/11] **[Test-time Backdoor Mitigation for Black-Box Large Language Models with Defensive Demonstrations](https://arxiv.org/abs/2311.09763)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Defense](https://img.shields.io/badge/Defense-87b800)
- [2023/10] **[Leveraging Diffusion-Based Image Variations for Robust Training on Poisoned Data](https://arxiv.org/abs/2310.06372)** ![Diffusion](https://img.shields.io/badge/Diffusion-a99cf4) ![NeurIPS'23_(Workshop)](https://img.shields.io/badge/NeurIPS'23_(Workshop)-f1b800)
- [2023/10] **[Large Language Models Are Better Adversaries: Exploring Generative Clean-Label Backdoor Attacks Against Text Classifiers](https://arxiv.org/abs/2310.18603)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![EMNLP'23_(Findings)](https://img.shields.io/badge/EMNLP'23_(Findings)-f1b800)
- [2023/10] **[PoisonPrompt: Backdoor Attack on Prompt-based Large Language Models](https://arxiv.org/abs/2310.12439)** [<img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" alt="Code" width="15" height="15">](https://github.com/grasses/PoisonPrompt) ![LLM](https://img.shields.io/badge/LLM-589cf4) ![ICASSP'24](https://img.shields.io/badge/ICASSP'24-f1b800)
- [2023/10] **[Prompt-Specific Poisoning Attacks on Text-to-Image Generative Models](https://arxiv.org/abs/2310.13828)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2023/10] **[Composite Backdoor Attacks Against Large Language Models](https://arxiv.org/abs/2310.07676)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2023/09] **[BadChain: Backdoor Chain-of-Thought Prompting for Large Language Models](https://openreview.net/forum?id=c93SBwz1Ma)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![ICLR'24](https://img.shields.io/badge/ICLR'24-f1b800)
- [2023/09] **[BadEdit: Backdooring Large Language Models by Model Editing](https://openreview.net/forum?id=duZANm2ABX)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![ICLR'24](https://img.shields.io/badge/ICLR'24-f1b800)
- [2023/09] **[Universal Jailbreak Backdoors from Poisoned Human Feedback](https://openreview.net/forum?id=GxCGsxiAaK)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![ICLR'24](https://img.shields.io/badge/ICLR'24-f1b800)
- [2023/08] **[LMSanitator: Defending Prompt-Tuning Against Task-Agnostic Backdoors](https://arxiv.org/abs/2308.13904)** [<img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" alt="Code" width="15" height="15">](https://github.com/meng-wenlong/LMSanitator) ![LLM](https://img.shields.io/badge/LLM-589cf4) ![NDSS'24](https://img.shields.io/badge/NDSS'24-f1b800)
- [2023/08] **[The Poison of Alignment](https://arxiv.org/abs/2308.13449)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2023/07] **[Backdooring Instruction-Tuned Large Language Models with Virtual Prompt Injection](https://arxiv.org/abs/2307.16888)** [<img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" alt="Code" width="15" height="15">](https://github.com/wegodev2/virtual-prompt-injection) ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2023/06] **[On the Exploitability of Instruction Tuning](https://arxiv.org/abs/2306.17194)** [<img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" alt="Code" width="15" height="15">](https://github.com/azshue/AutoPoison) ![LLM](https://img.shields.io/badge/LLM-589cf4) ![NeurIPS'23](https://img.shields.io/badge/NeurIPS'23-f1b800)
- [2023/05] **[Instructions as Backdoors: Backdoor Vulnerabilities of Instruction Tuning for Large Language Models](https://arxiv.org/abs/2305.14710)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2023/05] **[Poisoning Language Models During Instruction Tuning](https://arxiv.org/abs/2305.00944)** [<img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" alt="Code" width="15" height="15">](https://github.com/AlexWan0/Poisoning-Instruction-Tuned-Models) ![LLM](https://img.shields.io/badge/LLM-589cf4) ![ICML'23](https://img.shields.io/badge/ICML'23-f1b800)
- [2022/11] **[Rickrolling the Artist: Injecting Backdoors into Text Encoders for Text-to-Image Synthesis](https://arxiv.org/abs/2211.02408)** ![Diffusion](https://img.shields.io/badge/Diffusion-a99cf4) ![ICCV'23](https://img.shields.io/badge/ICCV'23-f1b800)




## Diffusion
