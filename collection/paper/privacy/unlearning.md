# C8. Unlearning

### Diffusion
- [2024/10] **[Meta-Unlearning on Diffusion Models: Preventing Relearning Unlearned Concepts](https://arxiv.org/abs/2410.12777)** ![Diffusion](https://img.shields.io/badge/Diffusion-a99cf4)
- [2024/10] **[Unstable Unlearning: The Hidden Risk of Concept Resurgence in Diffusion Models](https://arxiv.org/abs/2410.08074)** ![Diffusion](https://img.shields.io/badge/Diffusion-a99cf4)
- [2024/09] **[An Adversarial Perspective on Machine Unlearning for AI Safety](https://arxiv.org/abs/2409.18025)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/05] **[Defensive Unlearning with Adversarial Training for Robust Concept Erasure in Diffusion Models](https://arxiv.org/abs/2405.15234)** ![Diffusion](https://img.shields.io/badge/Diffusion-a99cf4)
- [2024/04] **[Espresso: Robust Concept Filtering in Text-to-Image Models](https://arxiv.org/abs/2404.19227)** ![Diffusion](https://img.shields.io/badge/Diffusion-a99cf4)
- [2024/03] **[Unveiling and Mitigating Memorization in Text-to-image Diffusion Models through Cross Attention](https://arxiv.org/abs/2403.11052)** ![Diffusion](https://img.shields.io/badge/Diffusion-a99cf4)
- [2024/03] **[MACE: Mass Concept Erasure in Diffusion Models](https://arxiv.org/abs/2403.06135)** ![Diffusion](https://img.shields.io/badge/Diffusion-a99cf4) ![CVPR'24](https://img.shields.io/badge/CVPR'24-f1b800)
- [2024/02] **[Machine Unlearning for Image-to-Image Generative Models](https://arxiv.org/abs/2402.00351)** [<img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" alt="Code" width="15" height="15">](https://github.com/jpmorganchase/l2l-generator-unlearning) ![Diffusion](https://img.shields.io/badge/Diffusion-a99cf4)
- [2023/09] **[Ring-A-Bell! How Reliable are Concept Removal Methods For Diffusion Models?](https://openreview.net/forum?id=lm7MRcsFiS)** ![Diffusion](https://img.shields.io/badge/Diffusion-a99cf4) ![Defense](https://img.shields.io/badge/Defense-87b800) ![ICLR'24](https://img.shields.io/badge/ICLR'24-f1b800)
- [2023/09] **[SalUn: Empowering Machine Unlearning via Gradient-based Weight Saliency in Both Image Classification and Generation](https://openreview.net/forum?id=gn0mIhQGNM)** ![Diffusion](https://img.shields.io/badge/Diffusion-a99cf4) ![ICLR'24_(Spotlight)](https://img.shields.io/badge/ICLR'24_(Spotlight)-f1b800)
- [2023/07] **[Right to be Forgotten in the Era of Large Language Models: Implications, Challenges, and Solutions](https://arxiv.org/abs/2307.03941)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2023/03] **[Erasing Concepts from Diffusion Models](https://arxiv.org/abs/2303.07345)** ![Diffusion](https://img.shields.io/badge/Diffusion-a99cf4)





### LLM 
- [2024/08] **[Forget to Flourish: Leveraging Machine-Unlearning on Pretrained Language Models for Privacy Leakage](https://arxiv.org/abs/2408.17354)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/08] **[Towards Robust Knowledge Unlearning: An Adversarial Framework for Assessing and Improving Unlearning Robustness in Large Language Models](https://arxiv.org/abs/2408.10682)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/08] **[Protecting Privacy Through Approximating Optimal Parameters for Sequence Unlearning in Language Models](https://aclanthology.org/2024.findings-acl.936/)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![ACL'24_(Findings)](https://img.shields.io/badge/ACL'24_(Findings)-f1b800)
- [2024/08] **[Towards Robust and Cost-Efficient Knowledge Unlearning for Large Language Models](https://arxiv.org/abs/2408.06621)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/07] **[Practical Unlearning for Large Language Models](https://arxiv.org/abs/2407.10223)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/07] **[UnUnlearning: Unlearning is not sufficient for content regulation in advanced generative AI](https://arxiv.org/abs/2407.00106)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/06] **[Intrinsic Evaluation of Unlearning Using Parametric Knowledge Traces](https://arxiv.org/abs/2406.11614)** [<img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" alt="Code" width="15" height="15">](https://github.com/yihuaihong/ConceptVectors) ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/06] **[RWKU: Benchmarking Real-World Knowledge Unlearning for Large Language Models](https://arxiv.org/abs/2406.10890)** [<img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" alt="Code" width="15" height="15">](https://rwku-bench.github.io/) ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/06] **[REVS: Unlearning Sensitive Information in Language Models via Rank Editing in the Vocabulary Space](https://arxiv.org/abs/2406.09325)** ![LLM](https://img.shields.io/badge/LLM-589cf4)

- [2024/04] **[Machine Unlearning in Large Language Models](https://arxiv.org/abs/2404.16841)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/04] **[LMEraser: Large Model Unlearning through Adaptive Prompt Tuning](https://arxiv.org/abs/2404.11056)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/04] **[Negative Preference Optimization: From Catastrophic Collapse to Effective Unlearning](https://arxiv.org/abs/2404.05868)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/04] **[Digital Forgetting in Large Language Models: A Survey of Unlearning Methods](https://arxiv.org/abs/2404.02062)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Survey](https://img.shields.io/badge/Survey-87b800)

- [2024/03] **[Localizing Paragraph Memorization in Language Models](https://arxiv.org/abs/2403.19851)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/03] **[Towards Efficient and Effective Unlearning of Large Language Models for Recommendation](https://arxiv.org/abs/2403.03536)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/03] **[Second-Order Information Matters: Revisiting Machine Unlearning for Large Language Models](https://arxiv.org/abs/2403.10557)** ![LLM](https://img.shields.io/badge/LLM-589cf4)

- [2024/03] **[Guardrail Baselines for Unlearning in LLMs](https://arxiv.org/abs/2403.03329)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/03] **[Dissecting Language Models: Machine Unlearning via Selective Pruning](https://arxiv.org/abs/2403.01267)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/02] **[Unmemorization in Large Language Models via Self-Distillation and Deliberate Imagination](https://arxiv.org/abs/2402.10052)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/02] **[Unlearnable Algorithms for In-context Learning](https://arxiv.org/abs/2402.00751)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/02] **[Towards Safer Large Language Models through Machine Unlearning](https://arxiv.org/abs/2402.10058)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/02] **[Machine Unlearning of Pre-trained Large Language Models](https://arxiv.org/abs/2402.15159)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![ACL'24](https://img.shields.io/badge/ACL'24-f1b800)
- [2024/02] **[Rethinking Machine Unlearning for Large Language Models ](https://arxiv.org/abs/2402.08787)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/02] **[Selective Forgetting: Advancing Machine Unlearning Techniques and Evaluation in Language Models](https://arxiv.org/abs/2402.05813)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/02] **[In-Context Learning Can Re-learn Forbidden Tasks](https://arxiv.org/abs/2402.05723)** ![LLM](https://img.shields.io/badge/LLM-589cf4)

- [2024/01] **[TOFU: A Task of Fictitious Unlearning for LLMs](https://arxiv.org/abs/2401.06121)** [<img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" alt="Code" width="15" height="15">](https://locuslab.github.io/tofu/) ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2023/10] **[In-Context Unlearning: Language Models as Few Shot Unlearners](https://arxiv.org/abs/2310.07579)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2023/10] **[Large Language Model Unlearning](https://arxiv.org/abs/2310.10683)** [<img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" alt="Code" width="15" height="15">](https://github.com/kevinyaobytedance/llm_unlearn) ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2023/10] **[Unlearn What You Want to Forget: Efficient Unlearning for LLMs](https://arxiv.org/abs/2310.20150)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2023/10] **[Who's Harry Potter? Approximate Unlearning in LLMs](https://arxiv.org/abs/2310.02238?s=08)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2023/09] **[Can Sensitive Information be Deleted From LLMs? Objectives for Defending Against Extraction Attacks](https://openreview.net/forum?id=7erlRDoaV8)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![ICLR'24_(Spotlight)](https://img.shields.io/badge/ICLR'24_(Spotlight)-f1b800)
- [2023/09] **[Detecting Pretraining Data from Large Language Models](https://openreview.net/forum?id=zWqr3MQuNs)** [<img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" alt="Code" width="15" height="15">](https://swj0419.github.io/detect-pretrain.github.io/) ![LLM](https://img.shields.io/badge/LLM-589cf4) ![ICLR'24](https://img.shields.io/badge/ICLR'24-f1b800)
